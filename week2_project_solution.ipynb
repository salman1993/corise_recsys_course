{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "837f0383-ee06-41a4-b431-032c431dfcbb",
   "metadata": {
    "id": "837f0383-ee06-41a4-b431-032c431dfcbb"
   },
   "source": [
    "# **Welcome to week 2 project!**\n",
    "\n",
    "Congratulations on making it to week 2! üëè In the first week of this course, we covered the basics of how to design personalized recommendation systems. We then provided some system design examples for large scale recommenders from corporations like Spotify and YouTube, as well as techniques for candidate generation, specifically the two-tower model being used at Twitter and Pinterest.\n",
    "\n",
    "This week, we covered details of ML approaches for recommendations: including multi-task recommenders and contextual bandits.\n",
    "\n",
    "In this week's project, we will touch upon both these methods at a high level. We first begin by developing a simple multi-task model, and then cover a contextual bandit model. For ease of use, we will re-use our H&M dataset from week 1 for Multi-task model, and will switch to using Yahoo! news dataset for contextual bandit model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a97fbf-e1ea-46f1-a355-fc1a4377a2ee",
   "metadata": {
    "id": "02a97fbf-e1ea-46f1-a355-fc1a4377a2ee"
   },
   "source": [
    "# **Multi-task Recommendations**\n",
    "\n",
    "Multi-task recommenders provide a way to predict multiple facets of user engagement in order to make comprehensive recommendations for content that users might like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3aeb2bd8-37fc-42f5-bc25-9c1eeb305d23",
   "metadata": {
    "id": "3aeb2bd8-37fc-42f5-bc25-9c1eeb305d23",
    "outputId": "ad3ee249-b33b-42e8-9dbe-7912458c7343"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/3688/1*rrIJOpJO8fkFECNHlwq-jQ.png\" width=\"500\" class=\"unconfined\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import Image, display\n",
    "display(Image(url='https://miro.medium.com/max/3688/1*rrIJOpJO8fkFECNHlwq-jQ.png', width=500, unconfined=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c659f20-fe0d-4c07-a8bd-6f5b90be571d",
   "metadata": {
    "id": "5c659f20-fe0d-4c07-a8bd-6f5b90be571d"
   },
   "source": [
    "\n",
    "**Single task recommender:** we will start by deveoping a single task model that makes predictions about whether the user will purchase the article or not.\n",
    "\n",
    "**Multi-task recommender:** we then expand the scope to consider auxilliary prediction tasks, in order to improve the recommendation performance.\n",
    "\n",
    "Specifically, we make the folliwing three predictions using a multi-task setup:\n",
    "1. Predicting whether the user will make any purchase next week or not\n",
    "2. Predicting which category the user will make purchase in\n",
    "3. Predicting which specific item the user will buy\n",
    "\n",
    "We will use different loss functions to jointly train the multi-task network.\n",
    "\n",
    "**Negative Sampling**\n",
    "It is important to note that the dataset is only of positive cases -- cases wherein the user bought an article. To train our models, we will have to resort to negative sampling to develop a mixed dataset comprising of positive and negative examples.\n",
    "\n",
    "We will cover two categories of negative sampling:\n",
    "1. Random negative sampling\n",
    "2. Negative sampling with bias\n",
    "\n",
    "**Evaluation:**\n",
    "We will evaluate the ranker performance on various ranking metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add75169-0279-469c-98c2-36e6fc28ebdd",
   "metadata": {
    "id": "add75169-0279-469c-98c2-36e6fc28ebdd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from typing import Dict, Text\n",
    "from functools import reduce\n",
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "from typing import Dict, Text\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow import feature_column\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3081bc7b-ab90-41c0-9c21-35dfe59b4228",
   "metadata": {
    "id": "3081bc7b-ab90-41c0-9c21-35dfe59b4228"
   },
   "outputs": [],
   "source": [
    "article_df = pd.read_csv(\"hmdata/articles.csv.zip\")\n",
    "customer_df = pd.read_csv(\"hmdata/customers.csv.zip\")\n",
    "train0 = pd.read_csv('hmdata/transactions_train.csv.zip')\n",
    "transaction_df = train0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c115e-84f5-419d-9337-5e00a434ff2f",
   "metadata": {
    "id": "340c115e-84f5-419d-9337-5e00a434ff2f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b073d71-0ffc-407c-9338-1a87cee00b07",
   "metadata": {
    "id": "2b073d71-0ffc-407c-9338-1a87cee00b07"
   },
   "source": [
    "# **Simple Feature Processing**\n",
    "\n",
    "In this section we focus on extracting features for trainnig the ranking models. We focus primarily on extracting user features, and article features, and leave sophisticated feature modelin for next week.\n",
    "\n",
    "For users specifically, we will focus on extracting the following features:\n",
    "1. age bucket\n",
    "2. no of past purchases\n",
    "3. min/max/avg price\n",
    "4. distrbution over product group\n",
    "5. distrbution over section\n",
    "\n",
    "For aticles, we will learn embedding based features for the article category, product group, and department.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b09610-07d8-4933-8bcd-1de5ec43193b",
   "metadata": {
    "id": "a3b09610-07d8-4933-8bcd-1de5ec43193b"
   },
   "outputs": [],
   "source": [
    "unique_customer_ids = customer_df.customer_id.unique()\n",
    "unique_article_ids = article_df.article_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8a544f-acd7-470d-8b51-1b6a9b051809",
   "metadata": {
    "id": "ca8a544f-acd7-470d-8b51-1b6a9b051809",
    "outputId": "9f864c9f-1ea7-4717-cbf1-e83359ca3fab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31788324 5108386\n"
     ]
    }
   ],
   "source": [
    "subset = train0[train0[\"t_dat\"]>\"2020-06-01\"]\n",
    "print(len(train0),len(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a5838-bc19-43bf-b31a-48bcd2578933",
   "metadata": {
    "id": "1e3a5838-bc19-43bf-b31a-48bcd2578933",
    "outputId": "f7b18fe1-7d88-48f5-a99d-f5a5f1a4f10c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def split(x):\n",
    "    if x[\"t_dat\"]<\"2020-08-22\":\n",
    "        return 1\n",
    "    return 0\n",
    "subset[\"train\"] = subset.apply (lambda row: split(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff690c-aef6-489f-8e13-037ed1c41084",
   "metadata": {
    "id": "01ff690c-aef6-489f-8e13-037ed1c41084"
   },
   "outputs": [],
   "source": [
    "joined = pd.merge(subset, article_df, on='article_id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e3302-0230-44a5-9fb4-847935fde83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d839a73-79fb-4364-897c-e8dd9607770b",
   "metadata": {
    "id": "4d839a73-79fb-4364-897c-e8dd9607770b"
   },
   "outputs": [],
   "source": [
    "f1 = joined.groupby(\"customer_id\", as_index=False).article_id.nunique() #unique articles\n",
    "f2 = joined.groupby(\"customer_id\", as_index=False).t_dat.nunique() #days active\n",
    "f3 = joined.groupby(\"customer_id\", as_index=False).size() #total purchases\n",
    "f4 = joined.groupby(\"customer_id\", as_index=False).price.min() #min price\n",
    "f5 = joined.groupby(\"customer_id\", as_index=False).price.max() #max price\n",
    "f6 = joined.groupby(\"customer_id\", as_index=False).price.mean() #mean price\n",
    "f7 = joined.groupby(\"customer_id\", as_index=False).product_group_name.nunique() #num unique product groups\n",
    "f8 = joined.groupby(\"customer_id\", as_index=False).department_name.nunique() #num unique department_name\n",
    "f9 = joined.groupby(\"customer_id\", as_index=False).section_name.nunique() #num unique section_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5eea45-0e60-48ef-8e48-90155fcdaf99",
   "metadata": {
    "id": "5c5eea45-0e60-48ef-8e48-90155fcdaf99"
   },
   "outputs": [],
   "source": [
    "data_frames = [f1,f2,f3,f4,f5,f6,f7,f8,f9]\n",
    "fAll = reduce(lambda  left,right: pd.merge(left,right,on=['customer_id'],\n",
    "                                            how='outer'), data_frames)\n",
    "fAll.columns = ['customer_id', 'nArticles', 'nDays', 'nPurchases', 'minPrice', 'maxPrice', 'meanPrice', 'nPGroups', 'nDept', 'nSect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3a6e5-aa2f-4f4f-aeca-b62c6f47e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fAll.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff03f5-65d4-4f91-98d8-68ccefbe48a2",
   "metadata": {
    "id": "e7ff03f5-65d4-4f91-98d8-68ccefbe48a2"
   },
   "outputs": [],
   "source": [
    "joinedF = pd.merge(joined, fAll, on='customer_id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74ea24-619d-4f02-a64e-b72c277ba9b7",
   "metadata": {
    "id": "2f74ea24-619d-4f02-a64e-b72c277ba9b7",
    "outputId": "dd5a1c6c-22a4-469f-8cd8-b630f4dfc074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t_dat', 'customer_id', 'article_id', 'price', 'sales_channel_id', 'train', 'product_code', 'prod_name', 'product_type_no', 'product_type_name', 'product_group_name', 'graphical_appearance_no', 'graphical_appearance_name', 'colour_group_code', 'colour_group_name', 'perceived_colour_value_id', 'perceived_colour_value_name', 'perceived_colour_master_id', 'perceived_colour_master_name', 'department_no', 'department_name', 'index_code', 'index_name', 'index_group_no', 'index_group_name', 'section_no', 'section_name', 'garment_group_no', 'garment_group_name', 'detail_desc', 'nArticles', 'nDays', 'nPurchases', 'minPrice', 'maxPrice', 'meanPrice', 'nPGroups', 'nDept', 'nSect']\n"
     ]
    }
   ],
   "source": [
    "print(list(joinedF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014bd67e-642d-4869-a079-8d2b4c94daeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5108386, 6), (5168036, 30), (5168036, 39))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.shape, joined.shape, joinedF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b579529e-69b8-4770-8759-fe8f77bf4d66",
   "metadata": {
    "id": "b579529e-69b8-4770-8759-fe8f77bf4d66",
    "outputId": "74b12cbb-d931-42e1-c75c-9185ef56be59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5108386, 15)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['customer_id','article_id',\n",
    "           'price',\n",
    "          'product_group_name', 'department_name',\n",
    "             'section_name',\n",
    "             'nArticles',\n",
    "             'nDays',\n",
    "             'nPurchases',\n",
    "             'minPrice',\n",
    "             'maxPrice',\n",
    "             'meanPrice',\n",
    "             'nPGroups',\n",
    "             'nDept',\n",
    "             'nSect']\n",
    "trainF = joinedF[columns][~joinedF[\"customer_id\"].isna()]\n",
    "trainF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e6078-f1ca-4993-ba1d-48a9dc94ea38",
   "metadata": {
    "id": "675e6078-f1ca-4993-ba1d-48a9dc94ea38"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>product_group_name</th>\n",
       "      <th>department_name</th>\n",
       "      <th>section_name</th>\n",
       "      <th>nArticles</th>\n",
       "      <th>nDays</th>\n",
       "      <th>nPurchases</th>\n",
       "      <th>minPrice</th>\n",
       "      <th>maxPrice</th>\n",
       "      <th>meanPrice</th>\n",
       "      <th>nPGroups</th>\n",
       "      <th>nDept</th>\n",
       "      <th>nSect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001f8cef6b9702d54abf66fd89eb21014bf98567065a9...</td>\n",
       "      <td>855834001</td>\n",
       "      <td>0.015831</td>\n",
       "      <td>Garment Full body</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Divided Collection</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.010153</td>\n",
       "      <td>0.027102</td>\n",
       "      <td>0.01713</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001f8cef6b9702d54abf66fd89eb21014bf98567065a9...</td>\n",
       "      <td>836130002</td>\n",
       "      <td>0.015831</td>\n",
       "      <td>Garment Full body</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Divided Collection</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.010153</td>\n",
       "      <td>0.027102</td>\n",
       "      <td>0.01713</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001f8cef6b9702d54abf66fd89eb21014bf98567065a9...</td>\n",
       "      <td>808841001</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>Garment Lower body</td>\n",
       "      <td>Skirts</td>\n",
       "      <td>Divided Collection</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.010153</td>\n",
       "      <td>0.027102</td>\n",
       "      <td>0.01713</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id  article_id     price  \\\n",
       "0  0001f8cef6b9702d54abf66fd89eb21014bf98567065a9...   855834001  0.015831   \n",
       "1  0001f8cef6b9702d54abf66fd89eb21014bf98567065a9...   836130002  0.015831   \n",
       "2  0001f8cef6b9702d54abf66fd89eb21014bf98567065a9...   808841001  0.016932   \n",
       "\n",
       "   product_group_name department_name        section_name  nArticles  nDays  \\\n",
       "0   Garment Full body         Dresses  Divided Collection        6.0    4.0   \n",
       "1   Garment Full body         Dresses  Divided Collection        6.0    4.0   \n",
       "2  Garment Lower body          Skirts  Divided Collection        6.0    4.0   \n",
       "\n",
       "   nPurchases  minPrice  maxPrice  meanPrice  nPGroups  nDept  nSect  \n",
       "0         6.0  0.010153  0.027102    0.01713       3.0    5.0    3.0  \n",
       "1         6.0  0.010153  0.027102    0.01713       3.0    5.0    3.0  \n",
       "2         6.0  0.010153  0.027102    0.01713       3.0    5.0    3.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainF.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff60881-c2ab-4c1f-b69a-67c28becb6cf",
   "metadata": {
    "id": "9ff60881-c2ab-4c1f-b69a-67c28becb6cf"
   },
   "source": [
    "# Negative Sampling for training ranker\n",
    "\n",
    "If you notice the dataset, we only have purchase information. This is generallyt he case -- we often only observe positive interactions from users. However, to train a model, we would need access to positive and negative samples both.\n",
    "\n",
    "To generate negative sampled, here we focus on random sampling of negative examples per user-artice transaction. Other methods exist to sample negatives, namely:\n",
    "1. Random sampling\n",
    "2. Biased sampling from similar categories\n",
    "3. Cross-batch negative sampling\n",
    "\n",
    "In this project, we will go ahead with the random sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf9e47-d778-4822-b37b-f0dd29323816",
   "metadata": {
    "id": "0ebf9e47-d778-4822-b37b-f0dd29323816"
   },
   "outputs": [],
   "source": [
    "tt = trainF.groupby(\"customer_id\", as_index=False).article_id.agg(['unique']).reset_index()\n",
    "tt.columns = ['customer_id', 'article_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcb19e5-9e0d-4fa8-ab65-b46ced061c35",
   "metadata": {
    "id": "cfcb19e5-9e0d-4fa8-ab65-b46ced061c35",
    "outputId": "66d3f8ce-721b-447f-fff8-657ed0876f0a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185914</th>\n",
       "      <td>4fea2883f9214209a27951728a06ed169b76f56e39c569...</td>\n",
       "      <td>[772773002, 372860069, 851530002, 817166007, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594229</th>\n",
       "      <td>ff011c9e7bb6348255867cb1cbeb6540dcfa7c073774f5...</td>\n",
       "      <td>[767862001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100509</th>\n",
       "      <td>2b192fdf426e0683e1a8174d9e0feaa6138f9ae4b0843e...</td>\n",
       "      <td>[833499002, 833530002, 684209019, 599580046, 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              customer_id  \\\n",
       "185914  4fea2883f9214209a27951728a06ed169b76f56e39c569...   \n",
       "594229  ff011c9e7bb6348255867cb1cbeb6540dcfa7c073774f5...   \n",
       "100509  2b192fdf426e0683e1a8174d9e0feaa6138f9ae4b0843e...   \n",
       "\n",
       "                                             article_list  \n",
       "185914  [772773002, 372860069, 851530002, 817166007, 5...  \n",
       "594229                                        [767862001]  \n",
       "100509  [833499002, 833530002, 684209019, 599580046, 6...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b22b40-e7c3-4b76-913f-9383119d114f",
   "metadata": {
    "id": "42b22b40-e7c3-4b76-913f-9383119d114f"
   },
   "outputs": [],
   "source": [
    "articlesSet = list(trainF.article_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f629c5-7b58-41ea-962d-ad559ee6197b",
   "metadata": {
    "id": "97f629c5-7b58-41ea-962d-ad559ee6197b",
    "outputId": "a653aeba-bc2c-4be0-a53e-754628539748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45892\n",
      "['customer_id', 'article_list']\n"
     ]
    }
   ],
   "source": [
    "print(len(articlesSet))\n",
    "tt.columns = ['customer_id', 'article_list']\n",
    "print(list(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d82cc39-6cdc-48d4-8a0b-1fba2d1171e5",
   "metadata": {
    "id": "8d82cc39-6cdc-48d4-8a0b-1fba2d1171e5"
   },
   "outputs": [],
   "source": [
    "def generate_negative_samples(train_df, article_list, num_neg=1):\n",
    "    negData = []\n",
    "    for index, row in train_df.iterrows():\n",
    "        allList = article_list\n",
    "        cust = row['customer_id']\n",
    "        art_list = row['article_list']\n",
    "        allList = list(set(allList) - set(art_list))\n",
    "        negs = random.sample(allList, num_neg)\n",
    "        for n in negs:\n",
    "            negData.append([cust,n])\n",
    "    negDf = pd.DataFrame(negData,columns=['customer_id','article_id'])\n",
    "    return negDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9828161-6e0f-4ada-b435-c35e5d401cd3",
   "metadata": {
    "id": "c9828161-6e0f-4ada-b435-c35e5d401cd3",
    "outputId": "5e33a41c-4a96-40cd-95a3-13ce3725f6fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26763</th>\n",
       "      <td>05b2245281021c4d8d747b9bfed040b18df3311f0ecc33...</td>\n",
       "      <td>924942001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58820</th>\n",
       "      <td>0c9e6d8dc0afcbee85e5f6f0989d2dec1ba9a6d4d9676e...</td>\n",
       "      <td>716590007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22169</th>\n",
       "      <td>04b552610f92d3ad694b8e9dd8d25356f46d9a9445c59e...</td>\n",
       "      <td>831998004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             customer_id  article_id\n",
       "26763  05b2245281021c4d8d747b9bfed040b18df3311f0ecc33...   924942001\n",
       "58820  0c9e6d8dc0afcbee85e5f6f0989d2dec1ba9a6d4d9676e...   716590007\n",
       "22169  04b552610f92d3ad694b8e9dd8d25356f46d9a9445c59e...   831998004"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negDf = generate_negative_samples(tt.head(40000), article_list=articlesSet, num_neg=2)\n",
    "print(negDf.shape)\n",
    "negDf.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affe3023-ad6e-4c30-b02d-df5626b479a5",
   "metadata": {
    "id": "affe3023-ad6e-4c30-b02d-df5626b479a5"
   },
   "source": [
    "### We next generate features for negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f652957e-c199-49db-a9f0-1268966ae57d",
   "metadata": {
    "id": "f652957e-c199-49db-a9f0-1268966ae57d",
    "outputId": "2648732f-ff17-4918-9d4b-51049d1de09f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>product_group_name</th>\n",
       "      <th>department_name</th>\n",
       "      <th>section_name</th>\n",
       "      <th>nArticles</th>\n",
       "      <th>nDays</th>\n",
       "      <th>nPurchases</th>\n",
       "      <th>minPrice</th>\n",
       "      <th>maxPrice</th>\n",
       "      <th>meanPrice</th>\n",
       "      <th>nPGroups</th>\n",
       "      <th>nDept</th>\n",
       "      <th>nSect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69128</th>\n",
       "      <td>0ed28e77a44be59f24a905737c2c84f152d49fd0377528...</td>\n",
       "      <td>843608002</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Scarves</td>\n",
       "      <td>Womens Big accessories</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.025407</td>\n",
       "      <td>0.042356</td>\n",
       "      <td>0.037460</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27429</th>\n",
       "      <td>05d6c73aa537db1008d7e1a64a985ffb896d81b3f0149c...</td>\n",
       "      <td>891025005</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>Blouse</td>\n",
       "      <td>Womens Everyday Collection</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>0.021533</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46316</th>\n",
       "      <td>09e3c3f5e38630278e7ea4d47b7d46c0a8ee568999f90e...</td>\n",
       "      <td>851137002</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>Kids Girl Knitwear</td>\n",
       "      <td>Kids Girl</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>0.042356</td>\n",
       "      <td>0.040237</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             customer_id  article_id  \\\n",
       "69128  0ed28e77a44be59f24a905737c2c84f152d49fd0377528...   843608002   \n",
       "27429  05d6c73aa537db1008d7e1a64a985ffb896d81b3f0149c...   891025005   \n",
       "46316  09e3c3f5e38630278e7ea4d47b7d46c0a8ee568999f90e...   851137002   \n",
       "\n",
       "       product_group_name     department_name                section_name  \\\n",
       "69128         Accessories             Scarves      Womens Big accessories   \n",
       "27429  Garment Upper body              Blouse  Womens Everyday Collection   \n",
       "46316  Garment Upper body  Kids Girl Knitwear                   Kids Girl   \n",
       "\n",
       "       nArticles  nDays  nPurchases  minPrice  maxPrice  meanPrice  nPGroups  \\\n",
       "69128          7      3           9  0.025407  0.042356   0.037460         2   \n",
       "27429          7      2           7  0.013542  0.030492   0.021533         1   \n",
       "46316          2      4           4  0.033881  0.042356   0.040237         1   \n",
       "\n",
       "       nDept  nSect  \n",
       "69128      5      3  \n",
       "27429      4      1  \n",
       "46316      2      2  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinedFN = pd.merge(negDf, article_df[['article_id','product_group_name', 'department_name','section_name']], on='article_id', how='left')\n",
    "joinedFN = pd.merge(joinedFN, fAll, on='customer_id', how='left')\n",
    "joinedFN.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40788433-0522-4e6a-a1ef-0a61b3670f50",
   "metadata": {
    "id": "40788433-0522-4e6a-a1ef-0a61b3670f50"
   },
   "outputs": [],
   "source": [
    "trainF['label1'] = 1\n",
    "joinedFN['label1'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaacc944-f2ed-4e94-afee-2599db25377a",
   "metadata": {
    "id": "aaacc944-f2ed-4e94-afee-2599db25377a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 16)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [trainF.head(40000), joinedFN]\n",
    "trainDF = pd.concat(data)\n",
    "trainDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294a2f5-4856-4317-ad4c-1bdf1e5066fb",
   "metadata": {
    "id": "3294a2f5-4856-4317-ad4c-1bdf1e5066fb",
    "outputId": "8837036e-a99c-4e84-d59f-b0dd6d7d04b4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>product_group_name</th>\n",
       "      <th>department_name</th>\n",
       "      <th>section_name</th>\n",
       "      <th>nArticles</th>\n",
       "      <th>nDays</th>\n",
       "      <th>nPurchases</th>\n",
       "      <th>minPrice</th>\n",
       "      <th>maxPrice</th>\n",
       "      <th>meanPrice</th>\n",
       "      <th>nPGroups</th>\n",
       "      <th>nDept</th>\n",
       "      <th>nSect</th>\n",
       "      <th>label1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24514</th>\n",
       "      <td>05381bc17e96de659d71f3cf501d41c5e6af2eb239971d...</td>\n",
       "      <td>904022001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Garment Lower body</td>\n",
       "      <td>Trousers &amp; Skirt</td>\n",
       "      <td>Womens Trend</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.022864</td>\n",
       "      <td>0.038119</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39462</th>\n",
       "      <td>5ef3a0112840e176322b8265561bb7d90921adc46f600a...</td>\n",
       "      <td>823505002</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>Swimwear</td>\n",
       "      <td>Swimwear</td>\n",
       "      <td>Womens Swimwear, beachwear</td>\n",
       "      <td>70.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.011847</td>\n",
       "      <td>0.101678</td>\n",
       "      <td>0.031409</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>0054c58805c2b6e64392099fa370c3becabfc1ee957b94...</td>\n",
       "      <td>853572001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Jewellery</td>\n",
       "      <td>Womens Small accessories</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>0.025407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             customer_id  article_id  \\\n",
       "24514  05381bc17e96de659d71f3cf501d41c5e6af2eb239971d...   904022001   \n",
       "39462  5ef3a0112840e176322b8265561bb7d90921adc46f600a...   823505002   \n",
       "1424   0054c58805c2b6e64392099fa370c3becabfc1ee957b94...   853572001   \n",
       "\n",
       "          price  product_group_name   department_name  \\\n",
       "24514       NaN  Garment Lower body  Trousers & Skirt   \n",
       "39462  0.016932            Swimwear          Swimwear   \n",
       "1424        NaN         Accessories         Jewellery   \n",
       "\n",
       "                     section_name  nArticles  nDays  nPurchases  minPrice  \\\n",
       "24514                Womens Trend        2.0    1.0         2.0  0.022864   \n",
       "39462  Womens Swimwear, beachwear       70.0   11.0        83.0  0.011847   \n",
       "1424     Womens Small accessories        2.0    1.0         2.0  0.016932   \n",
       "\n",
       "       maxPrice  meanPrice  nPGroups  nDept  nSect  label1  \n",
       "24514  0.038119   0.030492       2.0    2.0    2.0       0  \n",
       "39462  0.101678   0.031409       5.0   15.0    8.0       1  \n",
       "1424   0.033881   0.025407       1.0    2.0    1.0       0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b30d68-d8d0-4d94-8a6b-1e989b9d211b",
   "metadata": {
    "id": "d7b30d68-d8d0-4d94-8a6b-1e989b9d211b"
   },
   "outputs": [],
   "source": [
    "trainDF.to_pickle(\"trainDF.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9965cc2-bb5c-4ccd-8095-dac367deb68b",
   "metadata": {
    "id": "b9965cc2-bb5c-4ccd-8095-dac367deb68b",
    "outputId": "67c659bd-6c4e-4c21-a76b-786dbe40aa5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customer_id', 'article_id', 'price', 'product_group_name', 'department_name', 'section_name', 'nArticles', 'nDays', 'nPurchases', 'minPrice', 'maxPrice', 'meanPrice', 'nPGroups', 'nDept', 'nSect', 'label1']\n"
     ]
    }
   ],
   "source": [
    "print(list(trainDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e7128-8eee-4b15-a6dd-14b8ba62301c",
   "metadata": {
    "id": "857e7128-8eee-4b15-a6dd-14b8ba62301c"
   },
   "source": [
    "## Single Task Ranker\n",
    "\n",
    "We begin by training a single task ranker for the task of predicting whether the user will purchase a given article or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a069406-b11d-4e39-a902-a7e402c9c718",
   "metadata": {
    "id": "1a069406-b11d-4e39-a902-a7e402c9c718",
    "outputId": "3da6127c-5b7d-435f-e52b-8aed61b2c2c8",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "120/120 [==============================] - 1s 2ms/step - loss: 0.8045 - binary_accuracy: 0.6774\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.4938 - binary_accuracy: 0.8081\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.4592 - binary_accuracy: 0.8150\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.4332 - binary_accuracy: 0.8186\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.4148 - binary_accuracy: 0.8224\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.4034 - binary_accuracy: 0.8250\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3966 - binary_accuracy: 0.8266\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3919 - binary_accuracy: 0.8275\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3890 - binary_accuracy: 0.8263\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3873 - binary_accuracy: 0.8264\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3862 - binary_accuracy: 0.8256\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3852 - binary_accuracy: 0.8255\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3844 - binary_accuracy: 0.8254\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3836 - binary_accuracy: 0.8249\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3830 - binary_accuracy: 0.8248\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3822 - binary_accuracy: 0.8257\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3818 - binary_accuracy: 0.8253\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3815 - binary_accuracy: 0.8258\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3806 - binary_accuracy: 0.8258\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3804 - binary_accuracy: 0.8263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe99c3b1210>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numFeatures = ['nArticles','nDays','nPurchases','minPrice','maxPrice','meanPrice','nPGroups','nDept','nSect']\n",
    "catFeatures = ['product_group_name']\n",
    "X1 = np.asarray(trainDF[numFeatures].values)\n",
    "X2 = np.asarray(trainDF[catFeatures].values)\n",
    "y1 = np.asarray(trainDF['label1'].values)\n",
    "\n",
    "inp1 = tf.keras.Input((len(numFeatures)))\n",
    "#inp2 = tf.keras.Input((len(catFeatures)),dtype='str')\n",
    "#inp2 = tf.keras.Input((len(catFeatures)))\n",
    "\n",
    "unique_article_ids = article_df.article_id.unique().astype(str)\n",
    "\n",
    "\n",
    "x1 = tf.keras.layers.Dense(32, activation='relu')(inp1)\n",
    "embedding_dimension=50\n",
    "#xx2 = tf.keras.layers.StringLookup(vocabulary=unique_article_ids, mask_token=None)(inp2)\n",
    "#xx2 = tf.keras.layers.Embedding(len(unique_article_ids) + 1, embedding_dimension)(xx2)\n",
    "#xx2 = tf.keras.layers.Flatten()(xx2)\n",
    "#x2 = tf.keras.layers.Dense(32, activation='relu')(xx2)\n",
    "\n",
    "#x12 = tf.keras.layers.concatenate([x1,x2])\n",
    "#out1 = tf.keras.layers.Dense(1, activation='sigmoid')(x12)\n",
    "out1 = tf.keras.layers.Dense(1, activation='sigmoid')(x1)\n",
    "#out2 = tf.keras.layers.Dense(2, activation='softmax')(x123)\n",
    "#m = tf.keras.Model([inp,inp2,inp3], [out1,out2])\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "# metric = tf.keras.metrics.Accuracy()\n",
    "metric = tf.keras.metrics.BinaryAccuracy()\n",
    "m = tf.keras.Model([inp1], out1)\n",
    "m.compile(loss=bce,\n",
    "          optimizer='adam',\n",
    "          metrics=[metric])\n",
    "\n",
    "m.fit([X1], [y1], epochs=20, verbose=1,batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f7480-1848-4ec5-8384-e18eec2541f7",
   "metadata": {
    "id": "3d1f7480-1848-4ec5-8384-e18eec2541f7",
    "outputId": "4d9e3ec8-5341-44aa-e4df-db0db44909a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 9)]               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                320       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 353\n",
      "Trainable params: 353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486fbb9b-0e4c-4934-a17d-0d3d60295bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.3805 - binary_accuracy: 0.8258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.38048383593559265, 0.8258166909217834]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train set performance\n",
    "m.evaluate([X1], [y1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a584242-5f16-4c0f-b340-3351401f8d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unseen test set performance\n",
    "X1_test = np.asarray(trainF.tail(40000)[numFeatures].values) \n",
    "y1_test = trainF['label1'].tail(40000).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a31757-fd0c-4c58-80d3-0ccde975930e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 2s 2ms/step - loss: 3.6483 - binary_accuracy: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.6483004093170166, 0.0010000000474974513]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate([X1_test], [y1_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ada5f3-8157-4088-95b8-8e17989b2d52",
   "metadata": {
    "id": "65ada5f3-8157-4088-95b8-8e17989b2d52"
   },
   "source": [
    "### **From single to multi-task recommendation model**\n",
    "We can now add another task to enrich the single task model to multi-task model. To do so, we first have to decide what the task 2 would be.\n",
    "\n",
    "By looking at the data we have, we hypothesize that being able to predict whether or not a customer would purchase an article from certain product category would be a good learnt model to have. Indeed, being able to predict which category would the user purchase in would help us narrow down the list of recommendations we want to surface infront of users.\n",
    "\n",
    "Based on this intuition, we want to develop a multi-task model with 2 tasks:\n",
    "1. Task 1: predict whether or not a user would purchase an article\n",
    "2. Task 2: predict whether or not a user would purchase any article from a given category\n",
    "\n",
    "The training data we have created thus far is solely based on prediction task 1: predicting whether or not a user would purchase a given article. Corredpondingly, in trainDF dataframe we have collected positive and nagetive examples for this task, and have added a label1 column as the final label to train the model.\n",
    "\n",
    "To develop a multi-task model for these two tasks, we will now need to add a label for the second task in the same dataset: trainDF. To do so, we will need to add a column: label2 to this dataframe -- for each row, we will need to identify whether the user has made any purchase in this category or not, and if the user has made a purchase in this category, then we will assign label2 = 1, else label2 = 0. Please note that for the same training example, it may happen that label1 = 0 but label2 = 1; this would happen in cases wherein the customer did not purchase this specific article but did purchase any other article from this category.\n",
    "\n",
    "The goal for this project is to write the function that adds this label2 column to the trainDF dataframe, to enable us to train a multi-task model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b6f339-ccae-4cb8-949d-dc7947333bf1",
   "metadata": {
    "id": "39b6f339-ccae-4cb8-949d-dc7947333bf1"
   },
   "outputs": [],
   "source": [
    "def add_label2_to_trainDF():\n",
    "    \"\"\"\n",
    "    write code here to add a new column to trainDF\n",
    "    the new column would be named label2 and it represents the label for the task 2\n",
    "    \n",
    "    After this function has been run, it can be safely assumed that trainDF dataframe contains another column called \"label2\"\n",
    "    that describes the label for the second prediction task.\n",
    "    \"\"\"\n",
    "    \n",
    "    trainDF['label2'] = trainDF.product_group_name.apply(lambda s: 1 if s == \"Garment Upper body\" else 0)\n",
    "    \n",
    "add_label2_to_trainDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2642a24-090e-4180-9be0-e9bb27a89695",
   "metadata": {
    "id": "f2642a24-090e-4180-9be0-e9bb27a89695"
   },
   "source": [
    "Now that we have both the label1 and label2 in our dataset, we can construct a simple multi-task model with shared bottom, which we call the common layers. Building on top of the common layers, we will have two separate modules: task1 layers and task2 layers. Layers of task 1 (\"task1a\" and \"task1b\") are dedicated to learning weights that help us perform better on task1. Correspondingly, layers of task 2 (\"task2a\" and \"task2b\") are dedicated to learning weights that help us perform better on task2.\n",
    "\n",
    "Below we implement such a multi-task model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0790138b-204b-400e-b04d-73c605c4ea43",
   "metadata": {
    "id": "0790138b-204b-400e-b04d-73c605c4ea43",
    "outputId": "752aae33-e47a-4f51-f4be-20dff69c8774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "120/120 [==============================] - 1s 3ms/step - loss: 1.2038 - out1_loss: 0.5188 - out2_loss: 0.6850 - out1_binary_accuracy: 0.7825 - out2_binary_accuracy: 0.6138\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0480 - out1_loss: 0.3854 - out2_loss: 0.6626 - out1_binary_accuracy: 0.8261 - out2_binary_accuracy: 0.6227\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0404 - out1_loss: 0.3784 - out2_loss: 0.6620 - out1_binary_accuracy: 0.8261 - out2_binary_accuracy: 0.6225\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0373 - out1_loss: 0.3760 - out2_loss: 0.6614 - out1_binary_accuracy: 0.8287 - out2_binary_accuracy: 0.6227\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0384 - out1_loss: 0.3768 - out2_loss: 0.6615 - out1_binary_accuracy: 0.8275 - out2_binary_accuracy: 0.6224\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0362 - out1_loss: 0.3749 - out2_loss: 0.6613 - out1_binary_accuracy: 0.8286 - out2_binary_accuracy: 0.6228\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0345 - out1_loss: 0.3732 - out2_loss: 0.6614 - out1_binary_accuracy: 0.8294 - out2_binary_accuracy: 0.6223\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0345 - out1_loss: 0.3733 - out2_loss: 0.6612 - out1_binary_accuracy: 0.8296 - out2_binary_accuracy: 0.6223\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0336 - out1_loss: 0.3726 - out2_loss: 0.6610 - out1_binary_accuracy: 0.8298 - out2_binary_accuracy: 0.6225\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0323 - out1_loss: 0.3715 - out2_loss: 0.6608 - out1_binary_accuracy: 0.8304 - out2_binary_accuracy: 0.6223\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0322 - out1_loss: 0.3712 - out2_loss: 0.6610 - out1_binary_accuracy: 0.8300 - out2_binary_accuracy: 0.6224\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0321 - out1_loss: 0.3714 - out2_loss: 0.6607 - out1_binary_accuracy: 0.8297 - out2_binary_accuracy: 0.6227\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0306 - out1_loss: 0.3703 - out2_loss: 0.6603 - out1_binary_accuracy: 0.8305 - out2_binary_accuracy: 0.6227\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0305 - out1_loss: 0.3701 - out2_loss: 0.6604 - out1_binary_accuracy: 0.8306 - out2_binary_accuracy: 0.6227\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0301 - out1_loss: 0.3696 - out2_loss: 0.6605 - out1_binary_accuracy: 0.8306 - out2_binary_accuracy: 0.6227\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0297 - out1_loss: 0.3697 - out2_loss: 0.6600 - out1_binary_accuracy: 0.8313 - out2_binary_accuracy: 0.6228\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0288 - out1_loss: 0.3688 - out2_loss: 0.6600 - out1_binary_accuracy: 0.8310 - out2_binary_accuracy: 0.6227\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0284 - out1_loss: 0.3684 - out2_loss: 0.6600 - out1_binary_accuracy: 0.8320 - out2_binary_accuracy: 0.6226\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0278 - out1_loss: 0.3674 - out2_loss: 0.6603 - out1_binary_accuracy: 0.8321 - out2_binary_accuracy: 0.6227\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0269 - out1_loss: 0.3669 - out2_loss: 0.6600 - out1_binary_accuracy: 0.8318 - out2_binary_accuracy: 0.6227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe99c246890>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dimension=50\n",
    "\n",
    "numFeatures = ['nArticles','nDays','nPurchases','minPrice','maxPrice','meanPrice','nPGroups','nDept','nSect']\n",
    "catFeatures = ['product_group_name']\n",
    "X1 = np.asarray(trainDF[numFeatures].values)\n",
    "y1 = np.asarray(trainDF['label1'].values)\n",
    "y2 = np.asarray(trainDF['label2'].values) # replace label1 by label2 here\n",
    "\n",
    "inp1 = tf.keras.Input((len(numFeatures)))\n",
    "\n",
    "unique_article_ids = article_df.article_id.unique().astype(str)\n",
    "\n",
    "\n",
    "common = tf.keras.layers.Dense(128, activation='relu',name=\"common1\")(inp1)\n",
    "common = tf.keras.layers.Dense(64, activation='relu',name=\"common2\")(common)\n",
    "\n",
    "x1 = tf.keras.layers.Dense(32, activation='relu',name=\"task1a\")(common)\n",
    "x1 = tf.keras.layers.Dense(16, activation='relu',name=\"task1b\")(x1)\n",
    "out1 = tf.keras.layers.Dense(1, activation='sigmoid',name=\"out1\")(x1)\n",
    "\n",
    "x2 = tf.keras.layers.Dense(32, activation='relu',name=\"task2a\")(common)\n",
    "x2 = tf.keras.layers.Dense(16, activation='relu',name=\"task2b\")(x2)\n",
    "out2 = tf.keras.layers.Dense(1, activation='sigmoid',name=\"out2\")(x2)\n",
    "\n",
    "\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "metric = tf.keras.metrics.BinaryAccuracy()\n",
    "m2 = tf.keras.Model([inp1], [out1,out2])\n",
    "m2.compile(loss=bce,\n",
    "          optimizer='adam',\n",
    "          metrics=[metric])\n",
    "\n",
    "m2.fit([X1], [y1,y2], epochs=20, verbose=1,batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1dc2e8-e39d-4635-b5db-3fd9f36c0795",
   "metadata": {
    "id": "7a1dc2e8-e39d-4635-b5db-3fd9f36c0795",
    "outputId": "37f24ce2-334f-444e-d0bf-f20603899fa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " common1 (Dense)                (None, 128)          1280        ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " common2 (Dense)                (None, 64)           8256        ['common1[0][0]']                \n",
      "                                                                                                  \n",
      " task1a (Dense)                 (None, 32)           2080        ['common2[0][0]']                \n",
      "                                                                                                  \n",
      " task2a (Dense)                 (None, 32)           2080        ['common2[0][0]']                \n",
      "                                                                                                  \n",
      " task1b (Dense)                 (None, 16)           528         ['task1a[0][0]']                 \n",
      "                                                                                                  \n",
      " task2b (Dense)                 (None, 16)           528         ['task2a[0][0]']                 \n",
      "                                                                                                  \n",
      " out1 (Dense)                   (None, 1)            17          ['task1b[0][0]']                 \n",
      "                                                                                                  \n",
      " out2 (Dense)                   (None, 1)            17          ['task2b[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,786\n",
      "Trainable params: 14,786\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6bd1a-b8c7-41ba-9095-a060c4bf427e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750/3750 [==============================] - 9s 2ms/step - loss: 1.0259 - out1_loss: 0.3656 - out2_loss: 0.6603 - out1_binary_accuracy: 0.8328 - out2_binary_accuracy: 0.6223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0259010791778564,\n",
       " 0.3655661642551422,\n",
       " 0.660331666469574,\n",
       " 0.8328333497047424,\n",
       " 0.6222666501998901]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train set performance\n",
    "m2.evaluate([X1], [y1, y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ed11c-7c2a-40a4-bfcf-18098584a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainF['label2'] = trainF.product_group_name.apply(lambda s: 1 if s == \"Garment Upper body\" else 0)\n",
    "y2_test = trainF['label2'].tail(40000).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bff075-089a-4c12-91c9-b0984f824b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 3s 2ms/step - loss: 6.3744 - out1_loss: 5.6208 - out2_loss: 0.7536 - out1_binary_accuracy: 0.0010 - out2_binary_accuracy: 0.4388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.374423980712891,\n",
       " 5.6208367347717285,\n",
       " 0.753585159778595,\n",
       " 0.0010000000474974513,\n",
       " 0.43882501125335693]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unseen test set performance\n",
    "m2.evaluate([X1_test], [y1_test, y2_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5866181-5319-472a-8b85-0004fa8c652e",
   "metadata": {
    "id": "f5866181-5319-472a-8b85-0004fa8c652e"
   },
   "source": [
    "### Checkpoint & goals\n",
    "By now you should have implemented the function **def add_label2_to_trainDF()** and replaced label1 in the above code by label2 and trained the multi-task model with these 2 tasks.\n",
    "\n",
    "Please record the performance of both the single task model and multi-task model and provide your interpretation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9269deee-05ee-42c4-b8ce-7ee6b85d5b91",
   "metadata": {
    "id": "d030b3d7-9323-4ee1-8cf8-cc99c2048b44"
   },
   "source": [
    "##### Single task model:\n",
    "```\n",
    "Train set performance: loss: 0.3805 - binary_accuracy: 0.8258\n",
    "Test set performance:  loss: 3.6483 - binary_accuracy: 0.0010\n",
    "```\n",
    "\n",
    "##### Multi task model:\n",
    "```\n",
    "Train set performance: loss: 1.0259 - out1_loss: 0.3656 - out2_loss: 0.6603 - out1_binary_accuracy: 0.8328 - out2_binary_accuracy: 0.6223\n",
    "Test set performance:  loss: 6.3744 - out1_loss: 5.6208 - out2_loss: 0.7536 - out1_binary_accuracy: 0.0010 - out2_binary_accuracy: 0.4388\n",
    "```\n",
    "\n",
    "Both of the models seem to be performing similarly on Task 1 (predicting which article to buy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3554b0b-769d-4ded-81c2-9582e6450e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0478c09-91fa-404d-84f5-34ae7fe9f359",
   "metadata": {
    "id": "d0478c09-91fa-404d-84f5-34ae7fe9f359"
   },
   "source": [
    "## Freezing layers for secondary tasks\n",
    "\n",
    "Often when training a multi-task model, one needs to freeze parts of the nueral network and only train one specific task module, while keeping the weights of the other task module fixed. In Keras we can do this by freezing layers. The below example demonstrates how one could go about freezing layers in a neural network, and train only the trainable parts of a nueral network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab37c447-e7ba-451a-844a-8547ff227af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " common1 (Dense)                (None, 128)          1280        ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " common2 (Dense)                (None, 64)           8256        ['common1[0][0]']                \n",
      "                                                                                                  \n",
      " task1a (Dense)                 (None, 32)           2080        ['common2[0][0]']                \n",
      "                                                                                                  \n",
      " task2a (Dense)                 (None, 32)           2080        ['common2[0][0]']                \n",
      "                                                                                                  \n",
      " task1b (Dense)                 (None, 16)           528         ['task1a[0][0]']                 \n",
      "                                                                                                  \n",
      " task2b (Dense)                 (None, 16)           528         ['task2a[0][0]']                 \n",
      "                                                                                                  \n",
      " out1 (Dense)                   (None, 1)            17          ['task1b[0][0]']                 \n",
      "                                                                                                  \n",
      " out2 (Dense)                   (None, 1)            17          ['task2b[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,786\n",
      "Trainable params: 14,786\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40d34e-fa1c-4e09-b29a-7df832a3746b",
   "metadata": {
    "id": "3b40d34e-fa1c-4e09-b29a-7df832a3746b",
    "outputId": "0f71a6cb-20ab-40f5-e3ab-f190fa5fca9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.functional.Functional object at 0x7fe99c2a5b10> True\n",
      "<keras.engine.input_layer.InputLayer object at 0x7fe99c2d3c10> True\n",
      "<keras.layers.core.dense.Dense object at 0x7fe99c323190> True\n",
      "<keras.layers.core.dense.Dense object at 0x7fe9c7038790> True\n",
      "<keras.layers.core.dense.Dense object at 0x7fe99c26ef90> True\n",
      "<keras.layers.core.dense.Dense object at 0x7fe99c26ee50> True\n",
      "<keras.layers.core.dense.Dense object at 0x7fe9c6e5ded0> True\n",
      "<keras.layers.core.dense.Dense object at 0x7fe99c285a10> True\n",
      "<keras.layers.core.dense.Dense object at 0x7fe99c413d50> True\n",
      "<keras.layers.core.dense.Dense object at 0x7fe99c2a5e50> True\n"
     ]
    }
   ],
   "source": [
    "# lets print the training status of each layer of the models we have\n",
    "for k,v in m2._get_trainable_state().items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4842f73e-3367-4b47-890d-93936b18b212",
   "metadata": {
    "id": "4842f73e-3367-4b47-890d-93936b18b212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_7\n",
      "common1\n",
      "common2\n",
      "task1a\n",
      "task2a\n",
      "task1b\n",
      "task2b\n",
      "out1\n",
      "out2\n"
     ]
    }
   ],
   "source": [
    "# Lets try to freeze layers for task 2 so that no training example affects the module dedicated to task 2\n",
    "for layer in m2.layers:\n",
    "    print(layer.name)\n",
    "    if layer.name in [\"task2a\",\"task2b\"]:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b01277c-eb22-47b7-8bb7-7d1a106543c8",
   "metadata": {
    "id": "2b01277c-eb22-47b7-8bb7-7d1a106543c8",
    "outputId": "5e7f5cf3-8952-44e2-ee66-5e40e78019fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_6 True\n",
      "input_7 True\n",
      "common1 True\n",
      "common2 True\n",
      "task1a True\n",
      "task2a False\n",
      "task1b True\n",
      "task2b False\n",
      "out1 True\n",
      "out2 True\n"
     ]
    }
   ],
   "source": [
    "for k,v in m2._get_trainable_state().items():\n",
    "    print(k.name,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf305533-a7bf-4dd5-aa5d-c02f27f33897",
   "metadata": {
    "id": "cf305533-a7bf-4dd5-aa5d-c02f27f33897",
    "outputId": "fa2fc89c-94e8-4261-c3b4-dfe963a7a35b",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 3ms/step - loss: 1.0309 - out1_loss: 0.3711 - out2_loss: 0.6598 - out1_binary_accuracy: 0.8305 - out2_binary_accuracy: 0.6226\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0272 - out1_loss: 0.3676 - out2_loss: 0.6596 - out1_binary_accuracy: 0.8308 - out2_binary_accuracy: 0.6229\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0253 - out1_loss: 0.3656 - out2_loss: 0.6597 - out1_binary_accuracy: 0.8324 - out2_binary_accuracy: 0.6227\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0252 - out1_loss: 0.3655 - out2_loss: 0.6597 - out1_binary_accuracy: 0.8327 - out2_binary_accuracy: 0.6226\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0251 - out1_loss: 0.3654 - out2_loss: 0.6597 - out1_binary_accuracy: 0.8322 - out2_binary_accuracy: 0.6226\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0250 - out1_loss: 0.3655 - out2_loss: 0.6595 - out1_binary_accuracy: 0.8324 - out2_binary_accuracy: 0.6227\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0238 - out1_loss: 0.3643 - out2_loss: 0.6595 - out1_binary_accuracy: 0.8333 - out2_binary_accuracy: 0.6228\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0232 - out1_loss: 0.3635 - out2_loss: 0.6598 - out1_binary_accuracy: 0.8332 - out2_binary_accuracy: 0.6228\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0237 - out1_loss: 0.3641 - out2_loss: 0.6596 - out1_binary_accuracy: 0.8326 - out2_binary_accuracy: 0.6228\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0227 - out1_loss: 0.3631 - out2_loss: 0.6596 - out1_binary_accuracy: 0.8336 - out2_binary_accuracy: 0.6231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9646cea50>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.compile(loss=bce,\n",
    "          optimizer='adam',\n",
    "          metrics=[metric])\n",
    "\n",
    "m2.fit([X1], [y1,y2], epochs=10, verbose=1,batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b0509-3d51-46a6-84be-7a2069d0ad5f",
   "metadata": {
    "id": "9e9b0509-3d51-46a6-84be-7a2069d0ad5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 3s 2ms/step - loss: 6.6323 - out1_loss: 5.8702 - out2_loss: 0.7621 - out1_binary_accuracy: 5.7500e-04 - out2_binary_accuracy: 0.4388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.632340431213379,\n",
       " 5.870211124420166,\n",
       " 0.7621310949325562,\n",
       " 0.0005750000127591193,\n",
       " 0.43882501125335693]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unseen test set performance\n",
    "m2.evaluate([X1_test], [y1_test, y2_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cccb53-490d-48fe-89f9-99895bdfc50d",
   "metadata": {
    "id": "48cccb53-490d-48fe-89f9-99895bdfc50d"
   },
   "source": [
    "## Tasks for Week 2 project:\n",
    "\n",
    "So glad that you have made it this far!! The above gives a high level view on how one could train single, and multi-task models and how one could go about freezing layers and training parts of a multi-task network.\n",
    "\n",
    "Lets re-iterate the main goal for this part of the project 2:\n",
    "\n",
    "- Goal: Implement the function that assigns label2 to the trainDF dataframe, and run the single task and multi-tsk models without layer freezing. Once done, compare the performance metrics for task 1 from both these models. Comment on whether the addition of second task is adding value to predictive power of first task.\n",
    "\n",
    "Optional: you can notice that we have used only numeric features in the model. Feel free to add other categorical features and re-train the model to see if the performance increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a6c993-12a8-4600-9881-cd1c326d3859",
   "metadata": {
    "id": "06a6c993-12a8-4600-9881-cd1c326d3859"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d21b257c-d1e8-42c8-8926-86b8ee5ff88b",
   "metadata": {
    "id": "d21b257c-d1e8-42c8-8926-86b8ee5ff88b"
   },
   "source": [
    "# Contextual Bandits for Recommendations\n",
    "\n",
    "We can also use contextual bandits to produce personalized recommendations. Compared to multi-task recommenders, contextual bandits provide a balance between exploring new options and exploiting known information to find potential recommendations.\n",
    "\n",
    "## Background: multi-arm bandits\n",
    "The multi-armed bandit problem is an example of one-step reinforcement learning. To explore an example, lets assume we have a gambler who is provided a slot machine with multiple arms, each with its own unknown probability distribution of payouts. The objective is to pull the arms one-by-one in a sequence while gathering information in order to maximize the total payout over the long-run. The multi-armed bandit problem can be seen as a toy problem for reinforcement learning with one step rollout. Namely we have a game consisting of n rounds and in each round t:\n",
    "\n",
    "1. Player selects one of K actions (think of slot machines and pulling their arms, hence the name).\n",
    "2. Player gets reward of Rt . Each action i ‚àà {1,2,‚Ä¶,K } has a fixed, but unknown to the player, reward distribution Pi with the expected reward Œºi .\n",
    "3. Given the history of actions and rewards Player updates their strategy.\n",
    "\n",
    "Given the history of actions and rewards, the player updates their strategy. The goal of the player is to maximize the reward, which naturally has to be done by exploring new options (and thus learning about other machines' distributions) and exploiting known actions that proved to give high rewards so far. Mathematically the player wishes to minimize the regret.\n",
    "\n",
    "## Contextual multi-arm bandits\n",
    "The contextual bandit problem is a generalization of the multi-armed bandit that extends the model by making actions conditional on the state of the environment. Unlike the classical multi-armed bandit, it addresses the problem of identifying the most appropriate content given all relevant contextual signals.\n",
    "\n",
    "1. Player gets context xt .\n",
    "2. Player selects at ‚àà A .\n",
    "3. Player gets reward Rt(at) .\n",
    "4. Given the history of actions, rewards and contexts Player updates their strategy.\n",
    "\n",
    "A common real-world contextual bandit example is a recommendation system. Given a set of presented articles, a reward is determined by the click-through behavior of the user. If user clicks on the article, a payout of 1 is incurred and 0 otherwise. Click-through-rate (CRT) is used to determine the selection and placement of ads within the recommendation application.\n",
    "\n",
    "\n",
    "In explore phase the algorithm, given its internal estimates of goodness of each actions chooses the best possible action modulo exploration. For example, epsilon-greedy chooses the best action with probability (1‚àíœµ) and with probability œµ it selects uniformly one of all possible actions.\n",
    "\n",
    "In the learning phase, the algorithm, after it selects an action and obtains a reward, updates its estimates of expected rewards of actions given the context. This is usually done via feeding a single example consisting of x,a,r (context, action, reward) to an estimator model doing a single update step (batch size 1). The learn phase is usually common for most algorithms and the main differences come from the exploration phase. Some algorithms include œµ-greedy, ensemble of policies or approximations to Lin-UCB. \n",
    "œµ-greedy tends to be a practical choice for initial experiments since it‚Äôs straightforward to understand and control its exploration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc685243-6acc-460e-b6ba-15aa265daf97",
   "metadata": {
    "id": "dc685243-6acc-460e-b6ba-15aa265daf97"
   },
   "source": [
    "## Dataset: R6A - Yahoo! Front Page Today Module User Click Log Dataset, version 1.0 (1.1 GB)\n",
    "\n",
    "Our dataset contains a fraction of user click log for news articles displayed in the Featured Tab of the Today Module on Yahoo! Front Page (http://www.yahoo.com) during the first ten days in May 2009.  The articles were chosen uniformly at random from a hand-picked pool of high-quality articles, which allows one to use a recently developed method to obtain an unbiased evaluation of an arbitrary bandit algorithm.\n",
    "\n",
    "The dataset contains 45,811,883 user visits to the Today Module.  For each visit, both the user and each of the candidate articles are\n",
    "associated with a feature vector of dimension 6 (including a constant feature), constructed by conjoint analysis with a bilinear model.\n",
    "\n",
    "This dataset contains 10 files, corresponding to the first 10 days in May 2009:\n",
    "    ydata-fp-td-clicks-v1_0.20090501.gz\n",
    "    ydata-fp-td-clicks-v1_0.20090502.gz\n",
    "    ...\n",
    "    ydata-fp-td-clicks-v1_0.20090510.gz\n",
    "Each line in the files corresponds to a separate user visit.  An example line is as follows:\n",
    "\n",
    "1241160900 109513 0 |user 2:0.000012 3:0.000000 4:0.000006 5:0.000023 6:0.999958 1:1.000000 |109498 2:0.306008 3:0.000450 4:0.077048 5:0.230439 6:0.386055 1:1.000000 |109509 2:0.306008 3:0.000450 4:0.077048 5:0.230439 6:0.386055 1:1.000000 [[...more article features omitted...]] |109453 2:0.421669 3:0.000011 4:0.010902 5:0.309585 6:0.257833 1:1.000000\n",
    "\n",
    "which contains the following fields delimited with spaces:\n",
    "\n",
    "    * timestamp: e.g., 1241160900\n",
    "    * displayed_article_id: e.g., 109513\n",
    "    * user_click (0 for no-click and 1 for click): e.g., 0\n",
    "    * strings \"|user\" and \"|{article_id}\" indicate the start of user\n",
    "      and article features\n",
    "    * features are encoded as \"feature_id:feature_value\" pairs, and\n",
    "      feature_id starts from 1.\n",
    "\n",
    "The pool of available articles for recommendation for each user visit is the set of articles that appear in that line of data.  All user IDs (specifically, bcookies) are replaced by a common string 'user' so that no user information can be identified from this data.\n",
    "\n",
    "Each user or article is associated with six features.  Feature #1 is the constant (always 1) feature, and features #2-6 correspond to the 5 membership features constructed via conjoint analysis with a bilinear model.\n",
    "\n",
    "A unique property of this data set is that the displayed article is chosen uniformly at random from the candidate article pool.  Therefore, one can use an unbiased *offline* evaluation method to compare bandit algorithms in a reliable way.  Performance of some of the popular bandit algorithms can be found. We will cover the offline evaluation method in detail in week 4 of the course.\n",
    "\n",
    "The full dataset can be downloaded from:\n",
    "https://webscope.sandbox.yahoo.com/catalog.php?datatype=r&did=49\n",
    "We are going to work with a much smalled sample of this data, which can be downloaded from the Google drive link (data1.txt):\n",
    "https://drive.google.com/drive/folders/10LGZMgXRuz2qPr_QDbYdVVlKEcnQ25YL?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54535a2f-7d82-489b-aa3f-79d51e3d441e",
   "metadata": {
    "id": "54535a2f-7d82-489b-aa3f-79d51e3d441e"
   },
   "outputs": [],
   "source": [
    "files = (\"yahoo/data1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d57d965-645f-4622-b721-7cd11500fc67",
   "metadata": {
    "id": "8d57d965-645f-4622-b721-7cd11500fc67"
   },
   "source": [
    "Now lets write a function to read this file, and store the data in articles / features / events -- which we will treat as global variables accessible to all functions later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a362ca4c-5f65-4f51-99c7-43b62fe6043f",
   "metadata": {
    "id": "a362ca4c-5f65-4f51-99c7-43b62fe6043f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import fileinput\n",
    "\n",
    "def read_data(filenames):\n",
    "    \"\"\"\n",
    "    Reads a stream of events from the list of given files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filenames : list\n",
    "        List of filenames\n",
    "    \n",
    "    Stores\n",
    "    -------    \n",
    "    articles : [article_ids]\n",
    "    features : [[article_1_features] .. [article_n_features]]\n",
    "    events : [\n",
    "                 0 : displayed_article_index (relative to the pool),\n",
    "                 1 : user_click,\n",
    "                 2 : [user_features],\n",
    "                 3 : [pool_indexes]\n",
    "             ]\n",
    "    \"\"\"\n",
    "\n",
    "    global articles, features, events, n_arms, n_events\n",
    "    articles = []\n",
    "    features = []\n",
    "    events = []\n",
    "\n",
    "    skipped = 0\n",
    "\n",
    "    with fileinput.input(files=filenames) as f:\n",
    "        for line in f:\n",
    "            cols = line.split()\n",
    "            if (len(cols) - 10) % 7 != 0:\n",
    "                skipped += 1\n",
    "            else:\n",
    "                pool_idx = []\n",
    "                pool_ids = []\n",
    "                for i in range(10, len(cols) - 6, 7):\n",
    "                    id = cols[i][1:]\n",
    "                    if id not in articles:\n",
    "                        articles.append(id)\n",
    "                        features.append([float(x[2:]) for x in cols[i + 1: i + 7]])\n",
    "                    pool_idx.append(articles.index(id))\n",
    "                    pool_ids.append(id)\n",
    "\n",
    "                events.append(\n",
    "                    [\n",
    "                        pool_ids.index(cols[1]),\n",
    "                        int(cols[2]),\n",
    "                        [float(x[2:]) for x in cols[4:10]],\n",
    "                        pool_idx,\n",
    "                    ]\n",
    "                )\n",
    "    features = np.array(features)\n",
    "    n_arms = len(articles)\n",
    "    n_events = len(events)\n",
    "    print(n_events, \"events with\", n_arms, \"articles\")\n",
    "    if skipped != 0:\n",
    "        print(\"Skipped events:\", skipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888460da-4e01-4168-9573-761cf686786f",
   "metadata": {
    "id": "888460da-4e01-4168-9573-761cf686786f"
   },
   "source": [
    "Now lets read the file with the above defined function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df3304-6701-47ff-9d0d-d5a59a5b8cb3",
   "metadata": {
    "id": "07df3304-6701-47ff-9d0d-d5a59a5b8cb3",
    "outputId": "ac217ac4-2615-41c3-eac3-d3b30a1a6074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 events with 21 articles\n"
     ]
    }
   ],
   "source": [
    "read_data(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2039e-43ba-4a09-bbc9-b2b12687e722",
   "metadata": {
    "id": "95e2039e-43ba-4a09-bbc9-b2b12687e722"
   },
   "source": [
    "## œµ-greedy policy implementation\n",
    "Having read the files, now lets implement a simple œµ-greedy method. Here œµ defines the amount of exploration we wish to perform, and for the remaining 1-œµ times the model prefers exploitation. By convention, ‚Äúepsilon‚Äù represents the percentage of time/trials dedicated for exploration, and it is also typical to do random exploration. This introduces some form of stochasticity.\n",
    "\n",
    "The choose_arm function returns the best arm's index based on the œµ-greedy policy.\n",
    "The update functiom updates algorithm's parameters(matrices) for the selected arm. It looks at the selected arm, updates the number of times this arm was chosen, and then it updates the mean reward observed for the selected arm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ce40f-1fda-4397-ae7d-21f6faeacad9",
   "metadata": {
    "id": "d74ce40f-1fda-4397-ae7d-21f6faeacad9"
   },
   "outputs": [],
   "source": [
    "class Egreedy:\n",
    "    \"\"\"\n",
    "    Epsilon greedy algorithm implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        epsilon : number (Egreedy parameter, ideally between 0 and 1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.e = round(epsilon, 1)  # epsilon parameter for Egreedy \n",
    "        self.algorithm = \"Egreedy (Œµ=\" + str(self.e) + \")\"\n",
    "        self.q = np.zeros(n_arms)  # average reward for each arm -- this represents the known mean reward for each arm\n",
    "        self.n = np.zeros(n_arms)  # number of times each arm was chosen\n",
    "\n",
    "    def choose_arm(self, t, user, pool_idx):\n",
    "        \"\"\"\n",
    "        Returns the best arm's index relative to the pool\n",
    "        Parameters\n",
    "        ----------\n",
    "        t : number (number of trial)\n",
    "        user : array (user features)\n",
    "        pool_idx : array of indexes (pool indexes for article identification)\n",
    "        \"\"\"\n",
    "\n",
    "        p = np.random.rand()\n",
    "        if p > self.e:\n",
    "            # exploit\n",
    "            return np.argmax(self.q[pool_idx])\n",
    "        else:\n",
    "            # explore\n",
    "            return np.random.randint(low=0, high=len(pool_idx))\n",
    "\n",
    "    def update(self, displayed, reward, user, pool_idx):\n",
    "        \"\"\"\n",
    "        Updates algorithm's parameters(matrices)\n",
    "        Parameters\n",
    "        ----------\n",
    "        displayed : index (displayed article index relative to the pool)\n",
    "        reward : binary (user clicked or not)\n",
    "        user : array (user features)\n",
    "        pool_idx : array of indexes (pool indexes for article identification)\n",
    "        \"\"\"\n",
    "\n",
    "        a = pool_idx[displayed]\n",
    "        \n",
    "        # update counts pulled for chosen arm\n",
    "        self.n[a] += 1\n",
    "        \n",
    "        # update average/mean value/reward for chosen arm\n",
    "        self.q[a] += (reward - self.q[a]) / self.n[a]\n",
    "        \"\"\"\n",
    "        this can also be written as:\n",
    "        value = self.q[a]\n",
    "        new_value = ((self.n[a]-1)/float(self.n[a])) * value + (1 / float(self.n[a])) * reward\n",
    "        self.q[a] = new_value\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085cd6c7-f260-4c3d-9e66-333b06052a7c",
   "metadata": {
    "id": "085cd6c7-f260-4c3d-9e66-333b06052a7c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13ca41bc-b6b7-49c3-867b-50bc836ccae3",
   "metadata": {
    "id": "13ca41bc-b6b7-49c3-867b-50bc836ccae3"
   },
   "source": [
    "# Policy evaluation to evaluate contextual bandits\n",
    "\n",
    "We base our evaluation on the seminal work on offline evaluation of bandits as presented in:\n",
    "- [WSDM 2011] Unbiased Offline Evaluation of Contextual-bandit-based News Article Recommendation Algorithms\n",
    "https://arxiv.org/pdf/1003.5956.pdf\n",
    "\n",
    "Compared to machine learning in the more standard supervised learning setting, evaluation of methods in a contextual bandit setting is frustratingly difficult. Our goal here is to measure the performance of a bandit algorithm A, that is, a rule for selecting an arm at each time step based on the preceding interactions and current context.\n",
    "\n",
    "We suppose that there is some unknown distribution D from which tuples are drawn i.i.d. of the form (x, r1, . . . , rK), each consisting of observed context and unobserved payoffs for all arms. We also posit access to a long sequence of logged events resulting from the interaction of the uniformly random logging policy with the world. Each such event consists of the context vector x, a selected arm a, and the resulting observed payoff ra. Crucially, this logged data is partially labeled in the sense that only the payoff ra is observed for the single arm a that was chosen uniformly at random.\n",
    "\n",
    "Our goal is to use this data to evaluate a bandit algorithm A, which is a (possibly randomized) mapping for selecting the arm at at time t based on the history ht‚àí1 of t‚àí1 preceding events together with the current context. Therefore, the data serves as a benchmark, with which people can evaluate and compare different bandit algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854372ec-b76f-4c85-a30f-e303979cb9f2",
   "metadata": {
    "id": "854372ec-b76f-4c85-a30f-e303979cb9f2"
   },
   "source": [
    "The policy evaluator is shown in Algorithm 1 above. The method takes as input a bandit algorithm A and a desired\n",
    "number of ‚Äúvalid‚Äù events T on which to base the evaluation. We then step through the stream of logged events one by one. If, given the current history ht‚àí1, it happens that the policy A chooses the same arm a as the one that was selected by the logging policy, then the event is retained (that is, added to the history), and the total payoff updated. Otherwise, if the policy A selects a different arm from the one that was taken by the logging policy, then the event is entirely ignored, and the algorithm proceeds to the next event without any change in its state.\n",
    "\n",
    "We next implement this evaluation function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20a606-14d9-46b7-80d1-7e0f3bf70e76",
   "metadata": {
    "id": "ad20a606-14d9-46b7-80d1-7e0f3bf70e76"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3334e-253f-483c-9e39-4bdda2c3b00e",
   "metadata": {
    "id": "3eb3334e-253f-483c-9e39-4bdda2c3b00e"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "def evaluate(A, size=100, learn_ratio = 0.9):\n",
    "    \"\"\"\n",
    "    Policy evaluator as described in the paper\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : class (algorithm)\n",
    "    size : number (run the evaluation only on a portion of the dataset)\n",
    "    learn_ratio : number (perform learning(update parameters) only on a small portion of the traffic)\n",
    "    Returns\n",
    "    -------\n",
    "    learn : array (contains the ctr for each trial for the learning bucket)\n",
    "    deploy : array (contains the ctr for each trial for the deployment bucket)\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    # we initialize the payoff and events parameters separately for learning phase of the events and deployment phase of events.\n",
    "    Payoff_deploy = 0 # total payoff for the deployment bucket\n",
    "    Payoff_learn = 0  # total payoff for the learning bucket\n",
    "    Events_deploy = 1 # counter of valid events for the deployment bucket\n",
    "    Events_learn = 0  # counter of valid events for the learning bucket\n",
    "\n",
    "    learn = []\n",
    "    deploy = []\n",
    "    global events\n",
    "    if size != 100:\n",
    "        k = int(n_events * size / 100)\n",
    "        events = random.sample(events, k)\n",
    "\n",
    "    \"\"\"\n",
    "    we run through the logged events, and treat each event either for learning & updating the parameters,\n",
    "    or for deployment purposes wherein we use the reward obtained as evaluation metric\n",
    "    \"\"\"\n",
    "    for t, event in enumerate(events):\n",
    "\n",
    "        displayed = event[0]\n",
    "        reward = event[1]\n",
    "        user = event[2]\n",
    "        pool_idx = event[3]\n",
    "\n",
    "        # select the arm based on the bandit policy\n",
    "        chosen = A.choose_arm(Payoff_learn + Payoff_deploy, user, pool_idx)\n",
    "        \n",
    "        \"\"\"\n",
    "        If, given the current history ht‚àí1, it happens that the policy A chooses the same arm a\n",
    "        as the one that was selected by the logging policy, then the event is retained\n",
    "        (that is, added to the history), and the total payoff updated.\n",
    "        Otherwise, if the policy A selects a different arm from the one that was taken by the logging policy,\n",
    "        then the event is entirely ignored, and the algorithm proceeds to the next event without any change in its state.\n",
    "        \"\"\"\n",
    "        if chosen == displayed:\n",
    "            if random.random() < learn_ratio:\n",
    "                Payoff_learn += reward\n",
    "                Events_learn += 1\n",
    "                A.update(displayed, reward, user, pool_idx)\n",
    "                learn.append(Payoff_learn / Events_learn)\n",
    "            else:\n",
    "                Payoff_deploy += reward\n",
    "                Events_deploy += 1\n",
    "                deploy.append(Payoff_deploy / Events_deploy)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    execution_time = round(end - start, 1)\n",
    "    execution_time = (\n",
    "        str(round(execution_time / 60, 1)) + \"m\"\n",
    "        if execution_time > 60\n",
    "        else str(execution_time) + \"s\"\n",
    "    )\n",
    "    print(\n",
    "        \"{:<20}{:<10}{}\".format(\n",
    "            A.algorithm, round(Payoff_deploy / Events_deploy, 4), execution_time\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return learn, deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c803ab77-45b6-468d-94b1-5d386fbbdef5",
   "metadata": {
    "id": "c803ab77-45b6-468d-94b1-5d386fbbdef5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45aa4ece-69ca-4538-a88c-92e9b18a7510",
   "metadata": {
    "id": "45aa4ece-69ca-4538-a88c-92e9b18a7510"
   },
   "source": [
    "Now lets run the evaluation method on the epsilon-greedy policy for different values of epsilon, and print the corresponding reward obtained during deployment phase of evaluation.\n",
    "## Reward based evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f566f62-8a95-45b0-89da-8db6e9d00850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Egreedy (Œµ=0.1)     0.033     0.5s\n",
      "Egreedy (Œµ=0.1)     0.0343    0.5s\n",
      "Egreedy (Œµ=0.1)     0.0373    0.5s\n",
      "Egreedy (Œµ=0.2)     0.0291    0.5s\n",
      "Egreedy (Œµ=0.5)     0.0307    0.4s\n",
      "Egreedy (Œµ=0.8)     0.0297    0.4s\n"
     ]
    }
   ],
   "source": [
    "_, deploy = evaluate(Egreedy(0.1),learn_ratio=0.25)\n",
    "rnd_ctr = deploy[-1]\n",
    "\n",
    "_, deploy = evaluate(Egreedy(0.1),learn_ratio=0.5)\n",
    "rnd_ctr = deploy[-1]\n",
    "\n",
    "_, deploy = evaluate(Egreedy(0.1),learn_ratio=0.9)\n",
    "rnd_ctr = deploy[-1]\n",
    "\n",
    "_, deploy = evaluate(Egreedy(0.25),learn_ratio=0.5)\n",
    "rnd_ctr = deploy[-1]\n",
    "\n",
    "_, deploy = evaluate(Egreedy(0.5),learn_ratio=0.5)\n",
    "rnd_ctr = deploy[-1]\n",
    "\n",
    "_, deploy = evaluate(Egreedy(0.75),learn_ratio=0.5)\n",
    "rnd_ctr = deploy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00632099-405e-46f6-82d0-20e985ff08ab",
   "metadata": {
    "id": "00632099-405e-46f6-82d0-20e985ff08ab"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd0cfb8e-7c33-4f09-b917-ca26ea091142",
   "metadata": {
    "id": "bd0cfb8e-7c33-4f09-b917-ca26ea091142"
   },
   "source": [
    "# Goal 2: finish the implementation of UCB1 algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd37be-bf33-4613-b736-d674dc6d5889",
   "metadata": {
    "id": "b8bd37be-bf33-4613-b736-d674dc6d5889"
   },
   "source": [
    "## UCB-1 algorithm for arm selection\n",
    "\n",
    "Epsilon greedy performs pretty well, but it‚Äôs easy to see how selecting arms at random can be inefficient. If you have one movie that 50% of users have liked, and another at 5% have liked, epsilon greedy is equally likely to pick either of these movies when exploring random arms. Upper Confidence Bound algorithms were introduced as a class of bandit algorithm that explores more efficiently.\n",
    "\n",
    "Upper Confidence Bound algorithms construct a confidence interval of what each arm‚Äôs true performance might be, factoring in the uncertainty caused by variance in the data and the fact that we‚Äôre only able to observe a limited sample of pulls for any given arm. The algorithms then optimistically assume that each arm will perform as well as its upper confidence bound (UCB), selecting the arm with the highest UCB.\n",
    "\n",
    "### Motivation behind UCB\n",
    "Support an Artice A has been seen 100 times and has the best CTR. Article B has a slightly worse CTR than article A, but it hasn‚Äôt been seen by as many users, so there‚Äôs also more uncertainty about how well it‚Äôs going to perform in the long run. For this reason, it has a larger confidence bound, giving it a slightly higher UCB score than article A. Article C was published just moments ago, so almost no users have seen it. We‚Äôre extremely uncertain about how high its CTR will ultimately be, so its UCB is highest of all for now despite its initial CTR being low.\n",
    "\n",
    "Over time, more users will see articles B and C, and their confidence bounds will become more narrow and look more like that of article A. As we learn more about B and C, we‚Äôll shift from exploration toward exploitation as the articles‚Äô confidence intervals collapse toward their means. Unless the CTR of article B or C improves, the bandit will quickly start to favor article A again as the other articles‚Äô confidence bounds shrink."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209f8cb-1753-4f0c-b855-6ba8c1d99d9f",
   "metadata": {
    "id": "a209f8cb-1753-4f0c-b855-6ba8c1d99d9f"
   },
   "source": [
    "Let $n_t(a)$ be the number of times arm a is selected in rounds 1,2,‚Ä¶,t and $\\mu_t(a)$ be the average reward of arm a up to time t. The upper confidence bound is defined as:\n",
    "\n",
    "$UCB_t(a) = \\mu_t(a)+ \\sqrt{\\frac{2 \\log(t)}{ N_k(t)}}$\n",
    "\n",
    "\n",
    "where $\\mu_t(a)$ can be computed as `rewards[k] / pulls[k]` and the bound can be computed as `sqrt((2 * log(t)) / pulls[k]`.\n",
    "\n",
    "The UCB1 algorithm chooses the best arm based on an optimistic estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f5881aa5-6396-469d-a582-95af04e2224a",
   "metadata": {
    "id": "f5881aa5-6396-469d-a582-95af04e2224a"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.special import softmax\n",
    "\n",
    "class Ucb1:\n",
    "    def __init__(self, alpha):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha : number (ucb parameter)\n",
    "        \"\"\"\n",
    "\n",
    "        self.alpha = round(alpha, 2)\n",
    "        self.algorithm = \"UCB1 (Œ±=\" + str(self.alpha) + \")\"\n",
    "\n",
    "        self.q = np.zeros(n_arms)  # UCB for each arm\n",
    "        self.n = np.zeros(n_arms)  # number of times each arm was chosen\n",
    "        self.total_n = 0 # stores the total number of actions taken\n",
    "\n",
    "    def choose_arm(self, t, user, pool_idx):\n",
    "        \"\"\"\n",
    "        Returns the best arm's index relative to the pool\n",
    "        Parameters\n",
    "        ----------\n",
    "        t : number (number of trial)\n",
    "        user : array (user features)\n",
    "        pool_idx : array of indexes (pool indexes for article identification)\n",
    "        \"\"\"\n",
    "        \n",
    "        probs = softmax([self.q[i] for i in pool_idx])\n",
    "        single_sample = np.random.choice(pool_idx, size=1, replace=False, p=probs)\n",
    "        \n",
    "        return single_sample[0]\n",
    "\n",
    "    def update(self, displayed, reward, user, pool_idx):\n",
    "        \"\"\"\n",
    "        Updates algorithm's parameters(matrices)\n",
    "        Parameters\n",
    "        ----------\n",
    "        displayed : index (displayed article index relative to the pool)\n",
    "        reward : binary (user clicked or not)\n",
    "        user : array (user features)\n",
    "        pool_idx : array of indexes (pool indexes for article identification)\n",
    "        \"\"\"\n",
    "\n",
    "        a = pool_idx[displayed]\n",
    "        prev_avg_reward = self.q[a]\n",
    "        prev_num_pulls = self.n[a] \n",
    "        \n",
    "        # update self.n[a] here\n",
    "        self.n[a] += 1\n",
    "        self.total_n += 1\n",
    "        \n",
    "        # update self.q[a] here\n",
    "        curr_avg_reward = ((prev_avg_reward * prev_num_pulls) + reward) / self.n[a]\n",
    "        bound = math.sqrt((2 * math.log(self.total_n)) / self.n[a])\n",
    "        self.q[a] = curr_avg_reward + bound\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34852c-8ccf-4455-9863-5a5acc8e56f9",
   "metadata": {
    "id": "1d34852c-8ccf-4455-9863-5a5acc8e56f9"
   },
   "source": [
    "One of the most important features of the UCB is that it not only exponentially decays as the number of pulls on the given machine increases, but also increases as the timestep increases. In other words, arms that have been explored less are given a boost even if their estimated mean is low, especially if we‚Äôve been playing for a while. In this way, the UCB1 algorithm is able to naturally define its own mix of exploration vs. exploitation without depending on a user supplied parameter like epsilon greedy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e5976-b260-4df9-a00c-5ffe78ae46f0",
   "metadata": {
    "id": "289e5976-b260-4df9-a00c-5ffe78ae46f0"
   },
   "source": [
    "### Goal 2 for this week's project\n",
    "Finish the implementation of the UCB1 class above, and evaluate it for a set of parameters and compare its performance with respect to œµ-greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b5711ffd-2cab-43d1-8972-9b3b26cb14a4",
   "metadata": {
    "id": "b5711ffd-2cab-43d1-8972-9b3b26cb14a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCB1 (Œ±=1)          0.0252    9.7s\n",
      "UCB1 (Œ±=1)          0.0359    9.6s\n",
      "UCB1 (Œ±=1)          0.0194    9.9s\n"
     ]
    }
   ],
   "source": [
    "_, deploy = evaluate(Ucb1(alpha=1), learn_ratio=0.25)\n",
    "rnd_ctr = deploy[-1]\n",
    "\n",
    "_, deploy = evaluate(Ucb1(alpha=1), learn_ratio=0.5)\n",
    "rnd_ctr = deploy[-1]\n",
    "\n",
    "_, deploy = evaluate(Ucb1(alpha=1), learn_ratio=0.9)\n",
    "rnd_ctr = deploy[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca03492a-6db3-49fc-9040-95da0d0db68e",
   "metadata": {},
   "source": [
    "### Epsilon-greedy performance:\n",
    "```\n",
    "Egreedy (Œµ=0.1)     0.033     0.5s\n",
    "Egreedy (Œµ=0.1)     0.0343    0.5s\n",
    "Egreedy (Œµ=0.1)     0.0373    0.5s\n",
    "Egreedy (Œµ=0.2)     0.0291    0.5s\n",
    "Egreedy (Œµ=0.5)     0.0307    0.4s\n",
    "Egreedy (Œµ=0.8)     0.0297    0.4s\n",
    "```\n",
    "\n",
    "\n",
    "### UCB-1 performance over 5 trials:\n",
    "```\n",
    "UCB1 (Œ±=1)          0.0252    9.7s\n",
    "UCB1 (Œ±=1)          0.0359    9.6s\n",
    "UCB1 (Œ±=1)          0.0194    9.9s\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640d6cb-85bd-4ecc-8c67-90bffbe397b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "week2-multiTask-Bandit-final.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m89"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
